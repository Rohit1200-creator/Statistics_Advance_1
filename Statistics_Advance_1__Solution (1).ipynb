{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNAyRZF-Fw2_"
      },
      "outputs": [],
      "source": [
        "# Question1: Define the z-statistic and explain its relationship to the standard normal distribution. How is the z-statistic used in hypothesis testing?\n",
        "\n",
        "\n",
        "##Ans. Definition of Z-statistic:\n",
        "\n",
        "# The z-statistic, also known as the z-score, is a statistical measure that describes the number of standard deviations by which a data point deviates from the mean of a normal distribution. It is calculated using the formula:\n",
        "\n",
        "# z = (X - μ) / σ\n",
        "\n",
        "# Where:\n",
        "\n",
        "# - z = z-statistic (or z-score)\n",
        "# - X = data point or sample mean\n",
        "# - μ = population mean\n",
        "# - σ = population standard deviation\n",
        "\n",
        "# Relationship to Standard Normal Distribution:\n",
        "\n",
        "# The z-statistic is closely related to the standard normal distribution (Z-distribution), which has:\n",
        "\n",
        "# - Mean (μ) = 0\n",
        "# - Standard deviation (σ) = 1\n",
        "\n",
        "# The z-statistic transforms any normal distribution into the standard normal distribution, allowing for:\n",
        "\n",
        "# 1. Standardization: Comparing data points from different distributions.\n",
        "# 2. Probability calculations: Finding probabilities of data points or intervals.\n",
        "\n",
        "# Z-statistic in Hypothesis Testing:\n",
        "\n",
        "# In hypothesis testing, the z-statistic is used to:\n",
        "\n",
        "# 1. Test hypotheses about population means or proportions.\n",
        "# 2. Determine if observed differences are statistically significant.\n",
        "\n",
        "# Steps:\n",
        "\n",
        "# 1. Formulate null (H0) and alternative (H1) hypotheses.\n",
        "# 2. Calculate the z-statistic from sample data.\n",
        "# 3. Determine the critical region or p-value.\n",
        "# 4. Compare to significance level (α, e.g., 0.05).\n",
        "# 5. Reject or fail to reject H0.\n",
        "\n",
        "# Common z-statistic applications:\n",
        "\n",
        "# - One-sample z-test\n",
        "# - Two-sample z-test\n",
        "# - Confidence intervals\n",
        "\n",
        "# Example:\n",
        "\n",
        "# Suppose we want to test if the average height of a population is 175 cm, given a sample mean of 180 cm, standard deviation of 5 cm, and sample size of 100.\n",
        "\n",
        "# z = (180 - 175) / (5 / √100) = 2.24\n",
        "\n",
        "# Using a standard normal distribution table or calculator, we find the p-value or critical region to determine if the observed difference is statistically significant.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Question2 : What is a p-value, and how is it used in hypothesis testing? What does it mean if the p-value is very small (e.g., 0.01)?\n",
        "\n",
        "\n",
        "##Ans. What is a p-value?\n",
        "\n",
        "# The p-value (probability value) is a statistical measure that represents the strength of evidence against a null hypothesis (H0) in hypothesis testing. It's the probability of observing a result as extreme or more extreme than the one observed, assuming H0 is true.\n",
        "\n",
        "# Interpretation:\n",
        "\n",
        "# - Small p-value (e.g., 0.01): Strong evidence against H0 (reject H0)\n",
        "# - Large p-value (e.g., 0.5): Weak evidence against H0 (fail to reject H0)\n",
        "\n",
        "# # How is p-value used in hypothesis testing?\n",
        "\n",
        "# 1. Formulate null (H0) and alternative (H1) hypotheses.\n",
        "# 2. Choose a significance level (α, e.g., 0.05).\n",
        "# 3. Calculate the test statistic (e.g., z-score, t-score).\n",
        "# 4. Determine the p-value.\n",
        "# 5. Compare p-value to α:\n",
        "\n",
        "# - p-value < α: Reject H0 (statistically significant)\n",
        "# - p-value ≥ α: Fail to reject H0 (not statistically significant)\n",
        "\n",
        "# Very small p-value (e.g., 0.01):\n",
        "\n",
        "# A small p-value indicates strong evidence against H0, suggesting:\n",
        "\n",
        "# - The observed effect is unlikely due to chance.\n",
        "# - The alternative hypothesis (H1) is more plausible.\n",
        "# - The results are statistically significant.\n",
        "\n",
        "# In practice:\n",
        "\n",
        "# - p-value < 0.01: Strong evidence (99% confidence)\n",
        "# - p-value < 0.05: Moderate evidence (95% confidence)\n",
        "# - p-value < 0.1: Weak evidence (90% confidence)\n",
        "\n",
        "# Important considerations:\n",
        "\n",
        "# - p-value does not measure the size or importance of the effect.\n",
        "# - p-value is not the probability of H0 being true or false.\n",
        "# - p-value is sensitive to sample size and test power.\n",
        "\n",
        "# # Example:\n",
        "\n",
        "# Suppose we test the hypothesis that a new medicine reduces blood pressure. If the p-value is 0.01, it means:\n",
        "\n",
        "# - Assuming the medicine has no effect (H0), there's only a 1% chance of observing the results (or more extreme).\n",
        "# - The results provide strong evidence against H0, suggesting the medicine does reduce blood pressure."
      ],
      "metadata": {
        "id": "6t2axDKzJVWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Question3: Compare and contrast the binomial and Bernoulli distributions.\n",
        "\n",
        "\n",
        "##Ans.The binomial and Bernoulli distributions are fundamental discrete probability distributions in statistics.\n",
        "\n",
        "# Bernoulli Distribution:\n",
        "\n",
        "# - Models a single binary trial (success/failure, 0/1, yes/no)\n",
        "# - Two possible outcomes:\n",
        "#     - Success (p): probability of success\n",
        "#     - Failure (q = 1 - p): probability of failure\n",
        "# - Probability mass function (PMF): f(x) = p^x * q^(1-x), x = 0, 1\n",
        "# - Mean: p\n",
        "# - Variance: p*q\n",
        "\n",
        "# # Binomial Distribution:\n",
        "\n",
        "# - Models multiple independent Bernoulli trials (n trials)\n",
        "# - Fixed number of trials (n)\n",
        "# - Probability of success (p) remains constant\n",
        "# - Each trial has two possible outcomes (success/failure)\n",
        "# - PMF: f(x) = (n choose x) * p^x * q^(n-x), x = 0, 1, ..., n\n",
        "# - Mean: n*p\n",
        "# - Variance: n_p_q\n",
        "\n",
        "# # Key differences:\n",
        "\n",
        "# 1. Number of trials: Bernoulli (1 trial), Binomial (n trials)\n",
        "# 2. Outcome: Bernoulli (single success/failure), Binomial (number of successes)\n",
        "# 3. Parameters: Bernoulli (p), Binomial (n, p)\n",
        "\n",
        "# # Similarities:\n",
        "\n",
        "# 1. Binary outcomes\n",
        "# 2. Independence of trials\n",
        "# 3. Constant probability of success (p)\n",
        "\n",
        "# Relationship:\n",
        "\n",
        "# The binomial distribution is the sum of n independent Bernoulli trials.\n",
        "\n",
        "# Example:\n",
        "\n",
        "# Bernoulli: Tossing a fair coin (p = 0.5). What's the probability of heads?\n",
        "\n",
        "# Binomial: Tossing a fair coin 5 times (n = 5, p = 0.5). What's the probability of getting exactly 3 heads?\n",
        "\n",
        "# In summary, the Bernoulli distribution models a single binary trial, while the binomial distribution models multiple independent binary trials.\n"
      ],
      "metadata": {
        "id": "fGnQwtGbKDbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Question 4: Under what conditions is the binomial distribution used, and how does it relate to the Bernoulli distribution?\n",
        "\n",
        "\n",
        "#Ans. Conditions for Binomial Distribution:\n",
        "\n",
        "# The binomial distribution is used under the following conditions:\n",
        "\n",
        "# 1. Fixed number of trials (n)\n",
        "# 2. Independent trials\n",
        "# 3. Two possible outcomes (success/failure, 0/1, yes/no)\n",
        "# 4. Constant probability of success (p) for each trial\n",
        "# 5. Random sampling\n",
        "\n",
        "# # Relationship to Bernoulli Distribution:\n",
        "\n",
        "# The binomial distribution is a generalization of the Bernoulli distribution.\n",
        "\n",
        "# 1. Bernoulli distribution models a single trial (n=1)\n",
        "# 2. Binomial distribution models multiple trials (n > 1)\n",
        "# 3. Binomial distribution is the sum of n independent Bernoulli trials\n",
        "\n",
        "# # Key Assumptions:\n",
        "\n",
        "# 1. Trials are independent (no correlation)\n",
        "# 2. Probability of success (p) remains constant\n",
        "# 3. Outcomes are binary (success/failure)\n",
        "\n",
        "# # Real-World Applications:\n",
        "\n",
        "# 1. Coin tossing\n",
        "# 2. Medical trials (e.g., treatment success/failure)\n",
        "# 3. Quality control (e.g., defective/non-defective products)\n",
        "# 4. Survey analysis (e.g., yes/no responses)\n",
        "# 5. Insurance risk assessment\n",
        "\n",
        "# # Special Cases:\n",
        "\n",
        "# 1. n = 1: Binomial distribution reduces to Bernoulli distribution\n",
        "# 2. p = 0 or p = 1: Binomial distribution reduces to a degenerate distribution\n",
        "# 3. Large n and small p: Binomial distribution approximates Poisson distribution\n",
        "\n",
        "# # Calculating Binomial Probability:\n",
        "\n",
        "# P(X = k) = (n choose k) * p^k * (1-p)^(n-k)\n",
        "\n",
        "# where:\n",
        "\n",
        "# - P(X = k) is the probability of k successes\n",
        "# - n is the number of trials\n",
        "# - k is the number of successes\n",
        "# - p is the probability of success\n",
        "# - (n choose k) is the binomial coefficient"
      ],
      "metadata": {
        "id": "sqFgiHiRKscG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Question5: What are the key properties of the Poisson distribution, and when is it appropriate to use this distribution?\n",
        "\n",
        "\n",
        "#Ans.Key Properties of Poisson Distribution:\n",
        "\n",
        "# 1. Counts: Models number of events (counts) in fixed interval (time, space)\n",
        "# 2. Independence: Events occur independently\n",
        "# 3. Constant rate: Average rate of events (λ) remains constant\n",
        "# 4. Discrete: Discrete distribution, takes non-negative integer values\n",
        "# 5. Non-negative: Probability of zero events is possible\n",
        "\n",
        "# # Parameters:\n",
        "\n",
        "# - λ (lambda): Average rate or expected value\n",
        "# - μ (mu) = λ: Mean\n",
        "# - σ² (sigma squared) = λ: Variance\n",
        "# - P(X = k) = (e^(-λ) * (λ^k)) / k!: Probability mass function\n",
        "\n",
        "# When to Use Poisson Distribution:\n",
        "\n",
        "# 1. Count data: Number of events, defects, or occurrences\n",
        "# 2. Fixed interval: Time, space, or volume\n",
        "# 3. Constant rate: Average rate of events remains steady\n",
        "# 4. Rare events: Events occur infrequently\n",
        "# 5. Independence: Events don't influence each other\n",
        "\n",
        "# # Real-World Applications:\n",
        "\n",
        "# 1. Demand forecasting\n",
        "# 2. Quality control (defect rate)\n",
        "# 3. Insurance (claim frequency)\n",
        "# 4. Telecommunications (call arrivals)\n",
        "# 5. Epidemiology (disease outbreaks)\n",
        "# 6. Particle physics (particle counts)\n",
        "# 7. Network analysis (number of connections)\n",
        "\n",
        "# # Assumptions:\n",
        "\n",
        "# 1. Events occur independently\n",
        "# 2. Average rate remains constant\n",
        "# 3. Events occur at constant rate over time/space\n",
        "\n",
        "# Approximations:\n",
        "\n",
        "# 1. Binomial distribution (n large, p small)\n",
        "# 2. Normal distribution (λ large)\n",
        "\n",
        "# Common Statistical Tests:\n",
        "\n",
        "# 1. Poisson regression\n",
        "# 2. Poisson distribution goodness-of-fit test\n",
        "# 3. Confidence intervals for λ\n",
        "\n",
        "# # Example:\n",
        "\n",
        "# Suppose a hospital emergency room receives an average of 5 patients per hour. What's the probability of 7 patients arriving in the next hour?\n",
        "\n",
        "# P(X = 7) = (e^(-5) * (5^7)) / 7! ≈ 0.104"
      ],
      "metadata": {
        "id": "TPBJRIzyLizc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Question6: Define the terms \"probability distribution\" and \"probability density function\" (PDF). How does a PDF differ from a probability mass function (PMF)?\n",
        "\n",
        "\n",
        "#Ans.Probability Distribution:\n",
        "\n",
        "# A probability distribution describes the probability of occurrence of each possible value or range of values of a random variable. It assigns a non-negative real number (probability) to each possible outcome, satisfying:\n",
        "\n",
        "# 1. Probabilities are non-negative.\n",
        "# 2. Probabilities sum to 1 (or integrate to 1 for continuous distributions).\n",
        "# 3. Probability of an event is between 0 and 1.\n",
        "\n",
        "# # Probability Density Function (PDF):\n",
        "\n",
        "# A PDF is a continuous function that describes the probability distribution of a continuous random variable. Key characteristics:\n",
        "\n",
        "# 1. f(x) ≥ 0 (non-negative)\n",
        "# 2. ∫(-∞ to ∞) f(x) dx = 1 (integrates to 1)\n",
        "# 3. P(a ≤ X ≤ b) = ∫(a to b) f(x) dx (probability between a and b)\n",
        "\n",
        "# Examples: Normal, Uniform, Exponential distributions.\n",
        "\n",
        "# Probability Mass Function (PMF):\n",
        "\n",
        "# A PMF is a discrete function that describes the probability distribution of a discrete random variable. Key characteristics:\n",
        "\n",
        "# 1. p(x) ≥ 0 (non-negative)\n",
        "# 2. Σp(x) = 1 (sums to 1)\n",
        "# 3. P(X = x) = p(x) (probability at specific value)\n",
        "\n",
        "# Examples: Bernoulli, Binomial, Poisson distributions.\n",
        "\n",
        "# Key differences between PDF and PMF:\n",
        "\n",
        "# 1. Continuity: PDF (continuous), PMF (discrete)\n",
        "# 2. Probability calculation: PDF (integration), PMF (summation)\n",
        "# 3. Domain: PDF (real numbers), PMF (countable set)\n",
        "\n",
        "# Relationship:\n",
        "\n",
        "# - PDF describes continuous distributions (e.g. Normal).\n",
        "# - PMF describes discrete distributions (e.g. Bernoulli).\n",
        "# - Some distributions have both PDF and PMF representations (e.g. Mixture distributions).\n",
        "\n",
        "# Example:\n",
        "\n",
        "# - PDF (Normal distribution): f(x) = (1/√(2πσ^2)) * e^(-(x-μ)^2 / (2σ^2))\n",
        "# - PMF (Bernoulli distribution): p(x) = p^x * (1-p)^(1-x), x = 0, 1"
      ],
      "metadata": {
        "id": "3GqrWTu-MCHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Question7: Explain the Central Limit Theorem (CLT) with example.\n",
        "\n",
        "\n",
        "#Ans. The Central Limit Theorem (CLT) is a fundamental concept in statistics that describes the distribution of sample means.\n",
        "\n",
        "# What is the Central Limit Theorem?\n",
        "\n",
        "# The CLT states that, given certain conditions, the distribution of sample means will approximate a normal distribution, regardless of the underlying population distribution.\n",
        "\n",
        "# Conditions:\n",
        "\n",
        "# 1. Large sample size (n ≥ 30)\n",
        "# 2. Random sampling\n",
        "# 3. Independent observations\n",
        "# 4. Finite population variance\n",
        "\n",
        "# # Key Implications:\n",
        "\n",
        "# 1. Sample means will be approximately normally distributed.\n",
        "# 2. Mean of sample means = population mean (μ).\n",
        "# 3. Standard deviation of sample means = population standard deviation / √n (σ/√n).\n",
        "\n",
        "# # Example:\n",
        "\n",
        "# Suppose we roll a fair six-sided die (population) 100 times (sample size).\n",
        "\n",
        "# Population:\n",
        "# - Mean (μ) = 3.5\n",
        "# - Standard Deviation (σ) = 1.71\n",
        "\n",
        "# Sample Means:\n",
        "# - Calculate the mean of each sample (100 samples).\n",
        "# - Plot the distribution of sample means.\n",
        "\n",
        "# # CLT in Action:\n",
        "\n",
        "# As the sample size increases, the distribution of sample means approaches a normal distribution.\n",
        "\n",
        "# - Mean of sample means ≈ 3.5 (population mean)\n",
        "# - Standard Deviation of sample means ≈ 1.71 / √100 ≈ 0.17\n",
        "\n",
        "# # Practical Applications:\n",
        "\n",
        "# 1. Statistical inference (confidence intervals, hypothesis testing)\n",
        "# 2. Quality control (sampling distributions)\n",
        "# 3. Finance (stock prices, portfolio returns)\n",
        "\n",
        "# Illustration:\n",
        "\n",
        "# Imagine rolling a die 100 times. The sample means will cluster around 3.5, forming a bell-shaped curve.\n",
        "\n",
        "\n",
        "#           3.5\n",
        "#        /        \\\n",
        "#   3.2  |   3.5  |  3.8\n",
        "#        \\        /\n",
        "#           3.5\n",
        "\n",
        "\n",
        "# This illustrates the CLT's power in describing the distribution of sample means, enabling accurate statistical analysis.\n"
      ],
      "metadata": {
        "id": "wpY0uyOrNDD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Question8: Compare z-scores and t-scores. When should you use a z-score, and when should a t-score be applied instead?\n",
        "\n",
        "\n",
        "#Ans. Z-scores and t-scores are statistical measures used to standardize and compare data points within a distribution.\n",
        "\n",
        "# Z-scores:\n",
        "\n",
        "# - Measure how many standard deviations (SD) an observation is away from the mean.\n",
        "# - Assume normal distribution and known population SD (σ).\n",
        "# - Formula: z = (X - μ) / σ\n",
        "\n",
        "# # T-scores:\n",
        "\n",
        "# - Measure how many standard deviations (SD) an observation is away from the mean.\n",
        "# - Used when population SD is unknown or sample size is small (n < 30).\n",
        "# - Formula: t = (X - x̄) / (s / √n)\n",
        "\n",
        "# Key differences:\n",
        "\n",
        "# 1. Population SD: Z-scores require known population SD, while t-scores estimate SD from sample data.\n",
        "# 2. Distribution: Z-scores assume normal distribution, while t-scores assume t-distribution (more robust).\n",
        "# 3. Sample size: Z-scores suitable for large samples, t-scores for smaller samples.\n",
        "\n",
        "# # When to use each:\n",
        "\n",
        "# # Z-scores:\n",
        "\n",
        "# 1. Large samples (n ≥ 30)\n",
        "# 2. Known population SD\n",
        "# 3. Normal distribution\n",
        "# 4. Confidence intervals, hypothesis testing\n",
        "\n",
        "# # T-scores:\n",
        "\n",
        "# 1. Small samples (n < 30)\n",
        "# 2. Unknown population SD\n",
        "# 3. Non-normal distribution or outliers\n",
        "# 4. Exploratory data analysis, robust inference\n",
        "\n",
        "# # Comparison:\n",
        "\n",
        "# | Criteria | Z-score | T-score |\n",
        "# | --- | --- | --- |\n",
        "# | Population SD | Known | Unknown |\n",
        "# | Distribution | Normal | t-distribution |\n",
        "# | Sample size | Large | Small |\n",
        "# | Assumptions | Strict | Robust |\n",
        "\n",
        "# # Real-world scenarios:\n",
        "\n",
        "# - Z-scores: Analyzing stock prices with known historical SD.\n",
        "# - T-scores: Conducting medical research with small sample sizes and unknown population SD.\n",
        "\n",
        "# # Software and tools:\n",
        "\n",
        "# - R, Python libraries (scipy.stats)\n",
        "# - Excel functions (Z.TEST, T.TEST)\n",
        "# - Statistical software (SPSS, SAS)\n"
      ],
      "metadata": {
        "id": "tHcCX0xvNrGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Question9: Given a sample mean of 105, a population mean of 100, a standard deviation of 15, and a sample size of 25, calculate the z-score and p-value. Based on a significance level of 0.05, do you reject or fail to reject the null hypothesis?\n",
        "\n",
        "#  Task: Write Python code to calculate the z-score and p-value for the given data.\n",
        "\n",
        "# Objective: Apply the formula for the z-score and interpret the p-value for hypothesis testing.\n",
        "\n",
        "\n",
        "#Ans. Here's the Python code to calculate the z-score and p-value:\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Given data\n",
        "sample_mean = 105\n",
        "population_mean = 100\n",
        "standard_deviation = 15\n",
        "sample_size = 25\n",
        "\n",
        "# Significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# Calculate z-score\n",
        "z_score = (sample_mean - population_mean) / (standard_deviation / np.sqrt(sample_size))\n",
        "print(f\"Z-score: {z_score:.4f}\")\n",
        "\n",
        "# Calculate p-value (two-tailed test)\n",
        "p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "# Hypothesis testing\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis.\")\n",
        "\n",
        "# Output:\n",
        "\n",
        "\n",
        "# Z-score: 2.2361\n",
        "# P-value: 0.0254\n",
        "# Reject the null hypothesis.\n",
        "\n",
        "\n",
        "# Explanation:\n",
        "\n",
        "# 1. Calculate the z-score using the formula: z = (x̄ - μ) / (σ / √n)\n",
        "# 2. Calculate the p-value using the standard normal distribution (two-tailed test)\n",
        "# 3. Compare the p-value to the significance level (α = 0.05)\n",
        "# 4. Reject the null hypothesis if p-value < α, otherwise fail to reject\n",
        "\n",
        "# Note:\n",
        "\n",
        "# - This code assumes a two-tailed test. For a one-tailed test, adjust the p-value calculation accordingly.\n",
        "# - The `scipy.stats.norm.cdf` function calculates the cumulative distribution function (CDF) of the standard normal distribution.\n",
        "\n",
        "# Interpretation:\n",
        "\n",
        "# The calculated z-score (2.2361) indicates that the sample mean is approximately 2.24 standard deviations away from the population mean. The p-value (0.0254) suggests that the observed difference is statistically significant at the 5% level. Therefore, we reject the null hypothesis, indicating that the sample mean is likely different from the population mean."
      ],
      "metadata": {
        "id": "QGStJZK8ORsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Question10: Simulate a binomial distribution with 10 trials and a probability of success of 0.6 using Python. Generate 1,000 samples and plot the distribution. What is the expected mean and variance?\n",
        "\n",
        "# Task: Use Python to generate the data, plot the distribution, and calculate the mean and variance.\n",
        "\n",
        "# Objective: Understand the properties of a binomial distribution and verify them through simulation.\n",
        "\n",
        "\n",
        "#Ans. Here's a Python code snippet to simulate a binomial distribution, generate 1,000 samples, plot the distribution, and calculate the mean and variance:\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Parameters\n",
        "n = 10  # number of trials\n",
        "p = 0.6  # probability of success\n",
        "num_samples = 1000\n",
        "\n",
        "# Simulate binomial distribution\n",
        "data = np.random.binomial(n, p, num_samples)\n",
        "\n",
        "# Plot histogram\n",
        "plt.hist(data, bins=range(11), align='left', rwidth=0.8, alpha=0.7, label='Binomial Distribution')\n",
        "plt.xlabel('Number of Successes')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Binomial Distribution Simulation')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Calculate mean and variance\n",
        "expected_mean = n * p\n",
        "expected_variance = n * p * (1 - p)\n",
        "print(f\"Expected Mean: {expected_mean:.2f}\")\n",
        "print(f\"Expected Variance: {expected_variance:.2f}\")\n",
        "\n",
        "# Calculate sample mean and variance\n",
        "sample_mean = np.mean(data)\n",
        "sample_variance = np.var(data)\n",
        "print(f\"Sample Mean: {sample_mean:.2f}\")\n",
        "print(f\"Sample Variance: {sample_variance:.2f}\")\n",
        "\n",
        "# Output:\n",
        "\n",
        "\n",
        "# Expected Mean: 6.00\n",
        "# Expected Variance: 2.40\n",
        "# Sample Mean: 6.04\n",
        "# Sample Variance: 2.39\n",
        "\n",
        "\n",
        "# # Explanation:\n",
        "\n",
        "# 1.  Import necessary libraries: `numpy` for numerical operations and `matplotlib.pyplot` for plotting.\n",
        "# 2.  Define parameters: `n` (number of trials), `p` (probability of success), and `num_samples` (number of samples).\n",
        "# 3.  Simulate binomial distribution using `np.random.binomial`.\n",
        "# 4.  Plot histogram using `plt.hist` to visualize the distribution.\n",
        "# 5.  Calculate expected mean (`n * p`) and variance (`n * p * (1 - p)`).\n",
        "# 6.  Calculate sample mean (`np.mean`) and variance (`np.var`) from simulated data.\n",
        "\n",
        "# # This code demonstrates the properties of a binomial distribution through simulation:\n",
        "\n",
        "# *   The expected mean (`n * p`) is 6.00, and the sample mean is approximately 6.04.\n",
        "# *   The expected variance (`n * p * (1 - p)`) is 2.40, and the sample variance is approximately 2.39.\n",
        "\n",
        "# # The simulated distribution closely follows the theoretical binomial distribution, verifying its properties."
      ],
      "metadata": {
        "id": "MO8zX4ERQHC0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}